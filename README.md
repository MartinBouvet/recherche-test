# Moteur de Recherche Intelligent pour Mat√©riaux de Construction

Ce projet propose une solution de recherche intelligente pour un catalogue de mat√©riaux de construction. Contrairement aux moteurs de recherche classiques qui n√©cessitent des requ√™tes exactes, celui-ci comprend le langage naturel et s'adapte aux besoins r√©els des utilisateurs.

## Contexte et Objectifs

L'objectif √©tait de cr√©er un moteur de recherche capable de rivaliser avec des solutions professionnelles comme Algolia ou ElasticSearch, mais sans d√©pendre de services externes. Le d√©fi principal n'√©tait pas seulement d'int√©grer une interface IA, mais de rendre les donn√©es exploitables, coh√©rentes et rapidement consultables.

## Fonctionnalit√©s Principales

### Compr√©hension du Langage Naturel
Le syst√®me utilise un mod√®le LLM (Large Language Model) pour comprendre les intentions derri√®re les requ√™tes. Vous pouvez taper "je cherche du sable pour faire du b√©ton" et le syst√®me comprendra que vous recherchez des agr√©gats fins pour b√©ton, pas du sable de plage.

### Recherche Hybride
La recherche combine deux approches compl√©mentaires :
- **Recherche s√©mantique** : Utilise des embeddings pour trouver des produits similaires m√™me si les mots exacts ne sont pas pr√©sents
- **Recherche textuelle** : Recherche classique par mots-cl√©s avec un syst√®me de scoring intelligent

### Gestion des Fautes de Frappe
Le syst√®me corrige automatiquement les erreurs de saisie. Tapez "sabble" au lieu de "sable", et vous obtiendrez quand m√™me des r√©sultats pertinents.

### Synonymes M√©tier
Un dictionnaire de synonymes sp√©cialis√© permet de comprendre que "b√©ton" peut aussi signifier "ciment" ou "mortier" selon le contexte, ou que "pav√©" et "dalle" sont souvent interchangeables.

### Filtrage par Prix
Le syst√®me comprend les contraintes de budget exprim√©es naturellement. Par exemple, "carrelage gris ext√©rieur √† moins de 40 euros" filtrera automatiquement les r√©sultats selon le prix.

### Recherche Vocale
Une fonctionnalit√© de recherche vocale permet de dicter vos requ√™tes au lieu de les taper. Cliquez sur le bouton micro, parlez votre demande (ex: "je cherche des b√©tonni√®res de 25 L"), et le syst√®me transcrit automatiquement votre demande avant d'effectuer la recherche. Une phrase de confirmation personnalis√©e s'affiche ensuite pour confirmer la compr√©hension de votre demande.

### Performance Optimis√©e
- Cache des embeddings pour √©viter de les r√©g√©n√©rer √† chaque d√©marrage
- Cache des r√©sultats de recherche pour les requ√™tes fr√©quentes
- Recherche rapide (g√©n√©ralement moins de 200ms)

## Installation

### Pr√©requis

- Python 3.11 ou sup√©rieur
- Ollama install√© et configur√©
- Le mod√®le `nchapman/ministral-8b-instruct-2410:8b` disponible
- Un navigateur moderne avec support de l'API MediaRecorder (pour la recherche vocale)

### √âtapes d'Installation

1. **Cloner ou t√©l√©charger le projet**

2. **Cr√©er un environnement virtuel** (recommand√©) :
```bash
python -m venv venv
source venv/bin/activate  # Sur macOS/Linux
# ou
venv\Scripts\activate  # Sur Windows
```

3. **Installer les d√©pendances** :
```bash
pip install -r requirements.txt
```

4. **V√©rifier qu'Ollama est install√© et que le mod√®le est disponible** :
```bash
ollama list
# Si le mod√®le n'est pas pr√©sent :
ollama pull nchapman/ministral-8b-instruct-2410:8b
```

5. **Lancer Ollama** (dans un terminal s√©par√©) :
```bash
ollama serve
```

## Utilisation

### D√©marrer le Serveur

```bash
source venv/bin/activate  # Si vous utilisez un venv
python app.py
```

Le serveur d√©marre sur `http://localhost:8000` par d√©faut.

### Interface Web

Ouvrez simplement votre navigateur √† l'adresse `http://localhost:8000` et utilisez la barre de recherche. Vous pouvez formuler vos requ√™tes en langage naturel :

- "sable pour b√©ton"
- "isolation laine de roche"
- "pav√© terrasse"
- "carrelage gris ext√©rieur √† moins de 40 euros"
- "je veux faire un mur en b√©ton"

**Recherche vocale** : Cliquez sur le bouton micro üé§ √† c√¥t√© de la barre de recherche, parlez votre demande, puis cliquez √† nouveau pour arr√™ter l'enregistrement. Le syst√®me transcrit automatiquement votre demande et effectue la recherche.

### API REST

Le syst√®me expose une API REST simple :

**Recherche (GET)** :
```bash
curl "http://localhost:8000/api/search?q=carrelage%20ext√©rieur&limit=10"
```

**Recherche (POST)** :
```bash
curl -X POST "http://localhost:8000/api/search" \
  -H "Content-Type: application/json" \
  -d '{"query": "sable pour b√©ton", "limit": 10}'
```

**Transcription Audio** (POST) :
```bash
curl -X POST "http://localhost:8000/api/transcribe" \
  -F "audio=@recording.wav"
```

**Health Check** :
```bash
curl "http://localhost:8000/api/health"
```

## Architecture Technique

### Structure du Projet

- **`app.py`** : Application FastAPI principale, g√®re les endpoints et le cycle de vie
- **`data_loader.py`** : Charge et parse le fichier SQL, nettoie les donn√©es
- **`search_engine.py`** : C≈ìur du moteur de recherche avec LLM, embeddings et scoring
- **`synonyms.py`** : Dictionnaire de synonymes m√©tier et fonctions d'expansion
- **`templates/index.html`** : Interface web frontend
- **`analyze_relevance.py`** : Script d'analyse de la pertinence des r√©sultats
- **`check_images.py`** : Script de v√©rification des r√©f√©rences d'images

### Flux de Recherche

1. **Compr√©hension LLM** : Le mod√®le analyse la requ√™te et extrait :
   - Les termes techniques principaux
   - Les contraintes (prix, dimensions, etc.)
   - Une reformulation optimis√©e pour la recherche

2. **Correction des Fautes de Frappe** : Les termes sont compar√©s avec le vocabulaire du catalogue pour corriger les erreurs

3. **Expansion par Synonymes** : Les termes sont enrichis avec leurs synonymes m√©tier

4. **Recherche Hybride** :
   - **Textuelle (85%)** : Recherche par mots-cl√©s avec scoring intelligent
   - **S√©mantique (15%)** : Recherche par similarit√© d'embeddings

5. **Filtrage** : Application des contraintes (prix, score minimum)

6. **Tri et Retour** : R√©sultats tri√©s par pertinence

## Configuration

### Mod√®le LLM

Par d√©faut, le syst√®me utilise `nchapman/ministral-8b-instruct-2410:8b`. Pour changer de mod√®le, modifiez dans `search_engine.py` :

```python
model_name: str = "votre-modele"
```

### Mod√®le d'Embeddings

Le mod√®le par d√©faut est `paraphrase-multilingual-MiniLM-L12-v2` (multilingue, l√©ger, rapide). Pour changer :

```python
self.embedding_model = SentenceTransformer('votre-modele')
```

### Mod√®le Whisper (Recherche Vocale)

Le mod√®le par d√©faut est `base` (bon compromis vitesse/qualit√©). Pour am√©liorer la pr√©cision, vous pouvez utiliser un mod√®le plus grand :

**Mod√®les disponibles** (du plus rapide au plus pr√©cis) :
- `tiny` : Tr√®s rapide, moins pr√©cis (~39 Mo)
- `base` : Bon compromis, par d√©faut (~74 Mo)
- `small` : Meilleure qualit√©, un peu plus lent (~244 Mo)
- `medium` : Tr√®s bonne qualit√©, plus lent (~769 Mo)
- `large` : Meilleure qualit√©, beaucoup plus lent (~1550 Mo)

**Pour changer le mod√®le**, d√©finissez la variable d'environnement avant de lancer l'application :

```bash
export WHISPER_MODEL_SIZE=small
python app.py
```

Ou cr√©ez un fichier `.env` :
```
WHISPER_MODEL_SIZE=small
```

**Optimisations activ√©es** :
- Prompt initial avec vocabulaire m√©tier pour guider la transcription
- Param√®tres optimis√©s (temperature=0.0, beam_size=5, best_of=5)
- Post-traitement automatique pour corriger les termes techniques

### D√©sactiver le LLM

Pour des recherches ultra-rapides sans compr√©hension contextuelle, vous pouvez d√©sactiver le LLM :

```python
search_engine = SearchEngine(articles, use_llm=False)
```

## Gestion des Images

Les images sont r√©f√©renc√©es dans la base de donn√©es par leur nom de fichier. Pour qu'elles s'affichent correctement :

1. Cr√©ez un dossier `images/` √† la racine du projet :
```bash
mkdir images
```

2. Placez les images dans ce dossier avec les noms correspondants √† ceux de la base de donn√©es (ex: `49.jpg`, `2060.jpg`)

3. L'API servira automatiquement les images via l'endpoint `/images/{filename}`

Si une image n'existe pas, un placeholder sera affich√© automatiquement dans l'interface.

**Note** : Les noms de fichiers dans la base de donn√©es sont automatiquement nettoy√©s (quotes supprim√©es). Si vous voyez des erreurs 404, v√©rifiez que les noms correspondent exactement.

## Performance et Optimisations

### Cache des Embeddings

Les embeddings sont g√©n√©r√©s une premi√®re fois et sauvegard√©s dans `embeddings_cache.npy`. Au d√©marrage suivant, ils sont charg√©s depuis le cache, ce qui √©conomise environ 12-15 secondes.

### Cache des R√©sultats

Les r√©sultats de recherche sont mis en cache en m√©moire pour les requ√™tes identiques, permettant des r√©ponses quasi-instantan√©es pour les recherches r√©p√©t√©es.

### Temps de R√©ponse

- **Premier lancement** : ~30-60 secondes (g√©n√©ration des embeddings + chargement Whisper)
- **Lancements suivants** : ~2-5 secondes (chargement depuis cache)
- **Recherche** : ~80-200ms (selon la complexit√© de la requ√™te)
- **Transcription vocale** : ~1-3 secondes (selon la longueur de l'audio)

## Scripts Utiles

### Analyse de Pertinence

Pour analyser la qualit√© des r√©sultats et des donn√©es :

```bash
python analyze_relevance.py
```

Ce script permet de :
- V√©rifier la qualit√© des donn√©es charg√©es
- Tester la pertinence des recherches avec des requ√™tes pr√©d√©finies
- Analyser en d√©tail une requ√™te sp√©cifique

### V√©rification des Images

Pour voir quelles images sont r√©f√©renc√©es dans la base de donn√©es :

```bash
python check_images.py
```

## Limitations et Am√©liorations Futures

### Limitations Actuelles

- Le catalogue est charg√© en m√©moire (limit√© √† quelques dizaines de milliers d'articles)
- Pas de facettes avanc√©es (filtrage par cat√©gorie, marque, etc.)
- Le LLM peut parfois √™tre lent (d√©pend de votre configuration)

### Am√©liorations Possibles

1. **Base de donn√©es vectorielle** : Utiliser FAISS, Qdrant ou Pinecone pour g√©rer des millions d'articles
2. **Facettes et filtres** : Ajouter des filtres par cat√©gorie, prix, dimensions, etc.
3. **Historique et suggestions** : Sauvegarder les recherches fr√©quentes et proposer des suggestions
4. **A/B Testing** : Tester diff√©rents poids pour la recherche hybride
5. **Analytics** : Suivre les recherches les plus fr√©quentes et les produits les plus consult√©s

## D√©pannage

### Erreur "Model not found"

Assurez-vous qu'Ollama est lanc√© et que le mod√®le est disponible :
```bash
ollama serve
ollama pull nchapman/ministral-8b-instruct-2410:8b
```

### Erreur de chargement des donn√©es

V√©rifiez que le fichier SQL (`newflat_sograma_produits_seuls.sql`) est pr√©sent et lisible.

### Recherche lente

- R√©duisez le nombre de r√©sultats (`limit`)
- Augmentez `min_score` pour filtrer les r√©sultats peu pertinents
- D√©sactivez le LLM avec `use_llm=False` pour des recherches plus rapides

### Images qui ne s'affichent pas

- V√©rifiez que le dossier `images/` existe
- V√©rifiez que les noms de fichiers correspondent exactement (sans quotes)
- Utilisez `check_images.py` pour voir quelles images sont r√©f√©renc√©es

### Recherche vocale ne fonctionne pas

- **Whisper non charg√©** : V√©rifiez les logs au d√©marrage. Si Whisper ne se charge pas, la recherche vocale sera d√©sactiv√©e mais l'application fonctionnera normalement pour la recherche textuelle.
- **Permissions microphone** : Assurez-vous que votre navigateur a les permissions d'acc√®s au microphone. Dans Chrome/Edge, cliquez sur l'ic√¥ne de cadenas dans la barre d'adresse.
- **Format audio non support√©** : Whisper accepte plusieurs formats (WAV, MP3, WebM, OGG). Si vous avez des probl√®mes, essayez un autre navigateur.
- **Premier chargement lent** : Le mod√®le Whisper "base" fait environ 74 Mo et est t√©l√©charg√© automatiquement au premier lancement. Cela peut prendre quelques minutes selon votre connexion.
- **Transcription impr√©cise** : Si la transcription n'est pas assez pr√©cise, essayez d'utiliser un mod√®le plus grand (small, medium, ou large) via la variable d'environnement `WHISPER_MODEL_SIZE`. Notez que cela augmentera le temps de transcription et la consommation m√©moire.

## Contribution

Ce projet a √©t√© d√©velopp√© dans le cadre d'une d√©monstration de faisabilit√©. N'h√©sitez pas √† proposer des am√©liorations ou signaler des bugs !

## Licence

Ce projet est fourni tel quel, sans garantie particuli√®re.
